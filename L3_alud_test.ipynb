{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywPRt5z-TSmg"
   },
   "source": [
    "# Efficiency, Efficiency, Efficiency\n",
    "\n",
    "In this exercise, we'll take a crack at fine-tuning our very own LLM‚Äîhopefully increasing our efficiency as we go along.\n",
    "\n",
    "To run these models, we'll need access to an NVIDIA GPU with ~10 GiB of VRAM (or equivalent), so please run this notebook in one of the following environments (sorted by ease-of-use):\n",
    "* **Local Machine w/ GPU**: If you have your own machine with an appropriate GPU, just run it there.\n",
    "* **Google Colab**: Import this notebook to Colab, and set your runtime environment to \"T4 GPU\". Google provides some GPU time for free, but there are random limitations, so you might get demoted to a non-GPU environment at some point. Requires a Google Account.\n",
    "* **ITU's HPC**: You can set up Jupyter on a GPU node of the HPC and set up port-forwarding to your local machine ([official documentation](http://hpc.itu.dk/software/jupyternotebook/)).\n",
    "\n",
    "Once your environment is set up, let's get started by installing all the packages we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 34674,
     "status": "ok",
     "timestamp": 1735214835476,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "47NKnGnTTm5y"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q -U bitsandbytes  # provides model quantization\n",
    "!pip3 install -q -U peft  # provides hooks to turn our full model into a PEFT model\n",
    "!pip3 install -q -U trl  # provides methods to train our model\n",
    "!pip3 install -q -U accelerate  # provides methods for efficient GPU usage\n",
    "!pip3 install -q -U datasets  # provides easy access to large datasets\n",
    "!pip3 install -q -U transformers  # provides ready-to-run implementations of the most popular Transformer-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735214835476,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "LPZ2TAIwwWjS",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sq3ihuejDWl"
   },
   "source": [
    "## Time to Bloom üå∏!\n",
    "\n",
    "We'll be working with the Bloom series of models from the BigScience project ([BigScience, 2022](https://arxiv.org/abs/2211.05100)). Compared to the models from private companies such as LLaMA from Meta, or Gemma from Google, we know exactly what went into training the Bloom models, and can access them without having to accept these companies' privacy policies.\n",
    "\n",
    "By default, we'll be using Bloom's 1 billion parameter version, but the exercise should work with either of the smaller or larger versions as well, depending on your GPU size. Note that the even the relatively \"small\" 1B version is already almost 100x larger than the BERT-style models, which you might have used in previous courses. More common sizes of what people call LLMs now range from around 7B to 200B parameters, so keep that in mind while we go through our exercises.\n",
    "\n",
    "Anyway, let's load the model and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26049,
     "status": "ok",
     "timestamp": 1735214861522,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "QB6VkSQ1U2EX",
    "outputId": "929ea38a-2c91-43ea-a43f-77e4fe1cd6c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# uncomment the model you want to work with:\n",
    "# MODEL_ID = 'bigscience/bloom-560m'\n",
    "MODEL_ID = 'bigscience/bloom-1b1'\n",
    "# MODEL_ID = 'bigscience/bloom-3b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWX5fwew6Wmb"
   },
   "source": [
    "When you load the model for the first time, you might notice that it is loaded in \"shards\". This is to make file management easier, and is similar to how Transformers can be split up into multiple blocks. It is very common nowadays to have more than a dozen shards per model. After being downloaded, the shards are merged into one model in the machine's working memory.\n",
    "\n",
    "Let's check out the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1735214861522,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "e-x38C77YZZ_",
    "outputId": "f2b3416f-693e-4c0e-cdd4-76faf3039864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloomForCausalLM(\n",
      "  (transformer): BloomModel(\n",
      "    (word_embeddings): Embedding(250880, 1536)\n",
      "    (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x BloomBlock(\n",
      "        (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): BloomAttention(\n",
      "          (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "          (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): BloomMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (gelu_impl): BloomGelu()\n",
      "          (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFz48BwdwWjT"
   },
   "source": [
    "Notes:\n",
    "\n",
    "- Blooms nodel have 250.880 unique words while gpt-3 has around 50.000. It is because that Bloom is trained on a lot of languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuCvhwJR6fLP"
   },
   "source": [
    "### üìù Exercise 1\n",
    "\n",
    "Looks like a pretty standard Transformer architecture, right? Check your understanding of all components by answering the following questions:\n",
    "\n",
    "1. What's the difference between `BloomForCausalLM` and `BloomModel`?\n",
    "BloomModel is the base model. BloomForCausallM is the build on top of BloomModel for text generations (autoregressive) (next token prediction)\n",
    "\n",
    "2. How large is the vocabulary of this model?\n",
    "250.880\n",
    "\n",
    "3. How many layers does this model have?\n",
    "24 in total\n",
    "\n",
    "4. What do Bloom's feed-forward layers look like? I.e., are latent vectors up/down-scaled, and which non-linear activation function is used?\n",
    "dense_h_to_4h\n",
    "gelu_impl\n",
    "dense_4h_to_h\n",
    "\n",
    "6. All good with some self-attention, and MLPs, but what is all this other stuff: `LayerNorm`, `Dropout`? Do you also need to train these?\n",
    "LayerNorm: Normalizes input  / should be trained\n",
    "Dropout: Neurons set randomly to zero to prevent overfitting / should not be trained\n",
    "\n",
    "7. Bonus: What is happening with the query, key and value matrices in `query_key_value`? Why is this one parameter, and not three?\n",
    "For optimization purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. BloomModel is the base model. BloomForCausallM is the build on top of BloomModel for text generations (autoregressive) (next token prediction)\n",
    "\n",
    "2. 250.880\n",
    "\n",
    "3. 4 in total\n",
    "\n",
    "4. dense_h_to_4h, gelu_impl, dense_4h_to_h\n",
    "\n",
    "6. LayerNorm: Normalizes input  / should be trained. Dropout: Neurons set randomly to zero to prevent overfitting / should not be trained\n",
    "\n",
    "7. For optimization purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb8aXsYowWjT"
   },
   "source": [
    "Alright, now we got a good look at the model. Let's see how much space it's using up on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1735214861522,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "kfv999_RV2QO",
    "outputId": "9818f6ac-8aaa-436a-da8d-7e6ac5054a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 26 12:07:40 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   61C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCFaqzQ1wWjU"
   },
   "source": [
    "Huh? Considering that we made such a big deal about using GPUs this exercise, not much is happening on it üßê\n",
    "\n",
    "That's because by default, the model is loaded onto the CPU, and standard RAM. You can check where your model lives using the `*.device` property in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735214861522,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "35GIelA_G6cw",
    "outputId": "f1f2a558-69e4-45fa-971b-a12061053857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model lives on the cpu.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model lives on the {model.device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb3QQPluwWjU"
   },
   "source": [
    "Nothing against CPUs, but to go fast, we need to move over into CUDA-land on the GPU.\n",
    "\n",
    "Let's do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2098,
     "status": "ok",
     "timestamp": 1735214863617,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "AvlN_8dkHAjX",
    "outputId": "89d500bb-c245-44bf-d493-433a464bc0a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the model lives on the cuda:0 device.\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda')\n",
    "print(f\"Now the model lives on the {model.device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xnk1w5IHwWjU"
   },
   "source": [
    "Our model now lives on the 0th CUDA device, i.e., the one true GPU. We can verify this by directly checking the GPU usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1735214863617,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "v7DWpVOOHQUw",
    "outputId": "3d16c3f5-f732-4dbd-9de9-71d5a3746086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 26 12:07:42 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   62C    P0              31W /  70W |   4215MiB / 15360MiB |     33%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O80crax7wWjU"
   },
   "source": [
    "Bloom should now be cozily occupying ~4GBs of VRAM on our GPU. Lots of breathing room. Easy!\n",
    "\n",
    "To get a better idea of how the memory is being used, let's define a function to print out some stats regarding efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1735214863617,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "xotjTkN0YgFq",
    "outputId": "c2882700-6223-42e0-d456-54a906c56745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloomForCausalLM has:\n",
      "  1065314304 total parameters\n",
      "  1065314304 trainable parameters (100.00%)\n",
      "  4286423040 / 15835660288 bytes of VRAM in use (27.07%)\n"
     ]
    }
   ],
   "source": [
    "def print_model_statistics(model):\n",
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    cuda_total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    cuda_alloc_memory = torch.cuda.memory_allocated(0)\n",
    "    print(f\"{model.__class__.__name__} has:\")\n",
    "    print(f\"  {num_parameters} total parameters\")\n",
    "    print(f\"  {num_trainable_parameters} trainable parameters ({(num_trainable_parameters * 100)/num_parameters:.2f}%)\")\n",
    "    print(f\"  {cuda_alloc_memory} / {cuda_total_memory} bytes of VRAM in use ({(cuda_alloc_memory*100)/cuda_total_memory:.2f}%)\")\n",
    "\n",
    "print_model_statistics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anWI2lDXwWjU"
   },
   "source": [
    "This shows us in more detail, how all that VRAM is being used. We see that:\n",
    "\n",
    "* There are indeed one biiillliiiiooon ü§ô total parameters, which we enumerate using `model.parameters()`.\n",
    "* All of these parameters need to be trained, if we want to fine-tune the model. We count these by checking which of the above parameters require gradient computation, i.e., `p.requires_grad`.\n",
    "* How much memory PyTorch says it is using. Note that this includes not just the model, but also other code, that PyTorch needs to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQr3nCS7jM0n"
   },
   "source": [
    "## Let's Get Promptin'\n",
    "\n",
    "Now that we've gotten to know the model a bit better, let's do the generative AI thing: prompting.\n",
    "\n",
    "To make things easier, here's a little helper function.\n",
    "\n",
    "* It converts the input prompt into tokens, converts these tokens to numbers (i.e., token IDs) in the form of PyTorch tensors (`\"pt\"`), before moving them to the GPU. This is important, since the model lives on the GPU and needs to be able to access and work with this information.\n",
    "* Given the inputs, it then passes them iteratively through the model to generate one new token at a time. To not wait forever, we limit the number of new tokens to 64 (you can change this to get longer answers, if you'd like).\n",
    "* The outputs are also token IDs, so in the last step, we convert them back into strings by using the tokenizers `decode()` method. We skip special tokens, such as beginning of generation, end of generation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735214863617,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "yMFcqGlhZ0BY"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOG5EkeUwWjV"
   },
   "source": [
    "Alright, let's see what our model has to say!\n",
    "\n",
    "*(Generation is compute intensive, and may take around 5‚Äì30 seconds.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6476,
     "status": "ok",
     "timestamp": 1735214870090,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "kCtktwqEwWjV",
    "outputId": "5953845a-be75-418b-a82b-999b2cf3e5ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about the IT University of Copenhagen. What is it about?\n",
      "The IT University of Copenhagen is a unique university in Denmark. It is a unique university in the world. It is a unique university in Europe. It is a unique university in the world. It is a unique university in Europe. It is a unique university in the world. It is a unique\n"
     ]
    }
   ],
   "source": [
    "generate_response(\"Tell me about the IT University of Copenhagen.\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2TwsPHjwWjV"
   },
   "source": [
    "Bloom! üí• Now we're talking!\n",
    "\n",
    "Although the response sounds a little clunky, there's definitely... potential. Let's figure out what this model can do!\n",
    "\n",
    "### üìù Exercise 2\n",
    "\n",
    "1. Try out if the model responds correctly to the prompt, \"What is the capital of Denmark?\".\n",
    "2. The knowledge must be in there somewhere. Try to find a prompt, which results in the correct answer.\n",
    "3. Why might one prompt work, while the other does not?\n",
    "4. How good is the model's Danish?\n",
    "5. Bonus: Knock yourselves and try out some more prompts. Note down what the model might be better/worse at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4813,
     "status": "ok",
     "timestamp": 1735214874900,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "jCHeN24WwWjV",
    "outputId": "e0ecd5ae-49c8-4b9f-dd11-730af8403cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Denmark? Denmark is a country in the Baltic Sea, which is located in the northwestern part of Europe. Denmark is a country with a population of about 1.5 million people. Denmark is a country with a population of about 1.5 million people. Denmark is a country with a population of about 1.5 million people\n"
     ]
    }
   ],
   "source": [
    "#Q1\n",
    "generate_response(\"What is the capital of Denmark?\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1817,
     "status": "ok",
     "timestamp": 1735214876715,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "OG2fRSbywWjV",
    "outputId": "4bf73015-75da-4704-ce78-40b44835a6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denmark the country has a capital named? The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The capital of Denmark is Copenhagen. The\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "generate_response(\"Denmark the country has a capital named?\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735214876715,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "w57zNxLSwWjV"
   },
   "outputs": [],
   "source": [
    "#Q3\n",
    "#The first is more open-ended, why the other is better for next-token prediction (predicting the proability for the next word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2385,
     "status": "ok",
     "timestamp": 1735214879098,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "0dc1fVelwWjW",
    "outputId": "5dad5425-dd63-48fb-8370-007f05119f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den danske hovedstad hedder?\", \"Quel est le quartier le plus cher de Copenhague?\", \"Quel est le quartier le plus cher de Copenhague?\", \"Quel est le quartier le plus cher de Copenhague?\", \"Quel est le quartier le plus cher de Copenhague?\", \"Quel est le quartier le plus cher de Copenhague\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "generate_response(\"Den danske hovedstad hedder?\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735214879098,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "Aww31rzuwWjW"
   },
   "outputs": [],
   "source": [
    "# Actually answers the question, but in France. The model overall seems to vary very much on how you frame your prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhLPdOICjuM5"
   },
   "source": [
    "## No Train; No Gain\n",
    "\n",
    "The model obviously knows something, but it's hard to get useful answers from it. Actually, all LLMs start out this way, and they need additional training in-order to follow user instructions.\n",
    "\n",
    "### Extractive Question Answering\n",
    "\n",
    "Normally, we train autocomplete-style LLMs to become chat-style LLMs by applying instruction-tuning, multi-turn dialogue training, and alignment with human feedback. Unfortunately, we ain't got time for that (today). So, we're going to do something that is equivalent in-terms of method and format, but slightly smaller-scope: *extractive question answering*.\n",
    "\n",
    "In essence, instead of training the LLM to answer a question directly (because we would be training days on end), we will ask a question, and provide a context, which contains the answer. This way, the model has a better chance of finding the correct answer.\n",
    "\n",
    "To train the model, we first need the relevant training data, and luckily, we have the SQuAD v2 dataset ([Rajpurkar et al., 2018](https://aclanthology.org/P18-2124/)), which is publicly available on HuggingFace datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8233,
     "status": "ok",
     "timestamp": 1735214887329,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "5mq66o5RwWjW",
    "outputId": "93d79579-11a9-4b67-9db9-662e99a90d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SQuAD v2 dataset:\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 130319\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad_v2', split='train')\n",
    "print(f\"Loaded SQuAD v2 dataset:\\n{squad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3EyOrVMwWjW"
   },
   "source": [
    "130,319 rows of training data goodness (note that we only load the train split). This should be enough to train our LLM, and we'll actually only need to use part of it. While this might seem like a lot, full instruction tuning datasets contain *millions* of rows. Quite the high barrier to entry for new languages, isn't it?\n",
    "\n",
    "Now, what's actually in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1735214887329,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "YJAVAGM7wWjW",
    "outputId": "9bbbe088-8f9d-439b-9066-d1dd9ffd5131"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be9add3aeaaa14008c9152',\n",
       " 'title': 'Beyonc√©',\n",
       " 'context': 'Her fourth studio album 4 was released on June 28, 2011 in the US. 4 sold 310,000 copies in its first week and debuted atop the Billboard 200 chart, giving Beyonc√© her fourth consecutive number-one album in the US. The album was preceded by two of its singles \"Run the World (Girls)\" and \"Best Thing I Never Had\", which both attained moderate success. The fourth single \"Love on Top\" was a commercial success in the US. 4 also produced four other singles; \"Party\", \"Countdown\", \"I Care\" and \"End of Time\". \"Eat, Play, Love\", a cover story written by Beyonc√© for Essence that detailed her 2010 career break, won her a writing award from the New York Association of Black Journalists. In late 2011, she took the stage at New York\\'s Roseland Ballroom for four nights of special performances: the 4 Intimate Nights with Beyonc√© concerts saw the performance of her 4 album to a standing room only.',\n",
       " 'question': \"Beyonce's fourth album debuted in what year?\",\n",
       " 'answers': {'text': ['2011'], 'answer_start': [51]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[238]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZwVO5p7wWjW"
   },
   "source": [
    "The Queen herself! The format is pretty straightforward, but it's worth reflecting on how it came to üêù. SQuAD v1 and v2 are based on Wikipedia, so:\n",
    "\n",
    "* `title` is the title of the article;\n",
    "* `context` is a paragraph from the article;\n",
    "* `question` asks for information related to the article;\n",
    "* `answers` highlights one or more passages in the context, which contain the answer.\n",
    "\n",
    "Another cool thing about the second version of SQuAD in particular is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1735214887329,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "D1S9718zwWjX",
    "outputId": "e8e30a0b-cd23-4b28-dcd5-434c13e174be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5a8d7bf7df8bba001a0f9ab1',\n",
       " 'title': 'The_Legend_of_Zelda:_Twilight_Princess',\n",
       " 'context': 'The Legend of Zelda: Twilight Princess (Japanese: „Çº„É´„ÉÄ„ÅÆ‰ºùË™¨ „Éà„ÉØ„Ç§„É©„Ç§„Éà„Éó„É™„É≥„Çª„Çπ, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b]',\n",
       " 'question': 'What category of game is Legend of Zelda: Australia Twilight?',\n",
       " 'answers': {'text': [], 'answer_start': []}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[2075]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj9uTmnUwWjX"
   },
   "source": [
    "This question is related to the Legend of Zelda, but the answer is not in the context (because the name of the game is wrong). In these cases there are no answer segments. This is to test whether the model knows when it cannot know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la9MDiPrwWjX"
   },
   "source": [
    "### üìù Exercise 3\n",
    "\n",
    "Although it might look like natural language magic, when an LLM responds to an instruction, they are often trained to respond to instructions formatted in a specific format. For example, the popular Alpaca format is:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "```\n",
    "\n",
    "When providing a prompt through a user interface, it is typically reformatted into the format the LLM is used to before being passed to the model. If this is not done, responses may be worse or unpredictable.\n",
    "\n",
    "Let's come up with a good format for our extractive question answering task, by completing the formatting function below. Some pointers:\n",
    "* As input, this function takes one dataset row at a time (what you saw above by accessing `squad[i]`).\n",
    "* The formatted prompt should include all relevant information, i.e., context, question, and answer.\n",
    "* The last part should be the answer, such that the model can generate the answer to a new question, by continuing the provided input.\n",
    "* Each part should be easily differentiable from each other, using separators, which are unlikely to occurr in the input text.\n",
    "* No need to make the format overly complex, as it takes longer to generate (and train) on longer inputs.\n",
    "* Note that there should also be a default answer for when there is no answer (e.g., \"No idea, sorry :(\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735214887330,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "rbFjzsF8wWjX"
   },
   "outputs": [],
   "source": [
    "def format_instruction(row):\n",
    "\n",
    "    # If the field is empty, it returns a default string\n",
    "    context_ = row.get(\"context\", 'No context provided')\n",
    "    question_ = row.get(\"question\", 'No question provided')\n",
    "\n",
    "    answer_text = row.get(\"answers\", {}).get(\"text\", [])\n",
    "    if not answer_text:\n",
    "        answer_ = \"No answer provided\"\n",
    "    else:\n",
    "        answer_ = answer_text[0]\n",
    "\n",
    "    prompt = (\n",
    "        f\"### Context: {context_} \"\n",
    "        f\"### Question: {question_} \"\n",
    "        f\"### Answer: {answer_} \"\n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735214887330,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "gAF6B6WBwWjX",
    "outputId": "1fe16b1c-0af5-4f79-a40e-29d318334f20"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'### Context: Her fourth studio album 4 was released on June 28, 2011 in the US. 4 sold 310,000 copies in its first week and debuted atop the Billboard 200 chart, giving Beyonc√© her fourth consecutive number-one album in the US. The album was preceded by two of its singles \"Run the World (Girls)\" and \"Best Thing I Never Had\", which both attained moderate success. The fourth single \"Love on Top\" was a commercial success in the US. 4 also produced four other singles; \"Party\", \"Countdown\", \"I Care\" and \"End of Time\". \"Eat, Play, Love\", a cover story written by Beyonc√© for Essence that detailed her 2010 career break, won her a writing award from the New York Association of Black Journalists. In late 2011, she took the stage at New York\\'s Roseland Ballroom for four nights of special performances: the 4 Intimate Nights with Beyonc√© concerts saw the performance of her 4 album to a standing room only. ### Question: Which single had the most success from that album? ### Answer: Love on Top '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instruction(squad[239])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl-79MA7nDEP"
   },
   "source": [
    "## Full Fine-tuning\n",
    "\n",
    "We have our model; we have our data; we have our task formulation. Time to get down to business!\n",
    "\n",
    "Since the training objective itself is relatively simple‚Äîi.e., generate the next token (of the answer), based on the input context and question‚Äîwe don't need to implement any complex training loops, and can make use of the standard training helpers from HuggingFace. These are the Supervised Fine-tuning Trainer (`SFTTrainer`) and its acompanying configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2016,
     "status": "ok",
     "timestamp": 1735214889342,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "N9KNPNFgmDzg"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "trainer_config = SFTConfig(\n",
    "    output_dir='outputs',\n",
    "    learning_rate=5e-7,\n",
    "    lr_scheduler_type='constant',\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.001,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    max_seq_length=1024,\n",
    "    packing=True,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7q9_tMrlwWjY"
   },
   "source": [
    "### üìù Exercise 4\n",
    "\n",
    "We've set the hyperparameters to reasonable values, but make sure you understand what's happening here. The [official documentation](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTConfig) provides answers to some of these.\n",
    "\n",
    "1. The learning rate is set to 5x10^-7. Does this seem low or high to you? What might the reasons be?\n",
    "\n",
    "2. What do `per_device_train_batch_size` and `gradient_accumulation_steps` refer to? How many data points are loaded to the GPU, and how many account for one model weight update?\n",
    "\n",
    "3. How does the `max_seq_length` influence the efficiency of model training?\n",
    "\n",
    "4. What does the `fp16` flag change compared to the default training configuration?\n",
    "\n",
    "5. Bonus: What influence does the learning rate schedule have on model training, and what can we tweak here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 4\n",
    "\n",
    "We've set the hyperparameters to reasonable values, but make sure you understand what's happening here. The [official documentation](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTConfig) provides answers to some of these.\n",
    "\n",
    "1. The default is 2x10^-5 and is therefore low. I think it is to prevent overfitting and the model weight will not be influneced to much in each iteration\n",
    "2. Train batch size, Number of data points loaded for forward and backwards pass. How many forward and backwards occur before gradiants are used to update model.\n",
    "- 1 prompt/data point at a time. After 16 data point processing, it updates it weights. (16 account for one model weight update)\n",
    "\n",
    "3. If a lot of the input is larger than 1024 tokens, it makes the model more efficient as it truncates or cuts the rest of - also of the cost of losing important information.\n",
    "- If a lot of the input is smaller than 1024 tokens, it adds dummy tokens - using computation.\n",
    "\n",
    "4. It reduces the precession to 16-bit floating points, making the model smaller (less memory) and more efficient.\n",
    "\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC5H-z27wWjY"
   },
   "source": [
    "Alright, now let's put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1735214890367,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "Czc9mhd_n4We",
    "outputId": "68312756-3681-4178-f3f2-8985b46c8c7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-cf09045e9345>:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=squad,\n",
    "    formatting_func=format_instruction,\n",
    "    args=trainer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735215203389,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "IQOCuFPqyTmi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPpkhluOwWjY"
   },
   "source": [
    "As you can see (you can ignore the warnings), the trainer already pre-generates the training split by formatting the SQuAD dataset using our formatting function. Now we can get training by simply calling `trainer.train()`. Let's go! üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "error",
     "timestamp": 1735215208145,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "2bs6IU1w9GxA",
    "outputId": "9c4faca8-92a1-42ef-f00e-092077663150"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unexpected arguments: {'num_items_in_batch': 16384}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2522\u001b[0m                     )\n\u001b[1;32m   2523\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3706\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3707\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3708\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3709\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3710\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    967\u001b[0m             )\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeprecated_arguments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got unexpected arguments: {deprecated_arguments}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Got unexpected arguments: {'num_items_in_batch': 16384}"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRESfKrt-pL7"
   },
   "source": [
    "### üìù Exercise 5\n",
    "\n",
    "Oh no! The dreaded `OutOfMemoryError: CUDA out of memory.`! Why does this happen? Let's check the state of our GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1735215202849,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "3DPdtMRO9xm4"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Lr4QJNwWjZ"
   },
   "source": [
    "Hmm, curious... looks like the GPU is indeed full. Let's clear out the trash from the GPU's VRAM for a second. Unfortunately, Jupyter doesn't make this easy, so please comment out the `trainer.train()` command above and `Restart Kernel and Run Up to Selected Cell...`.\n",
    "\n",
    "While the notebook is re-doing its thing, let's think about why this may be happening:\n",
    "\n",
    "**Question**: Why does the GPU fill up, although we could easily run inference on it just a moment before?\n",
    "\n",
    "**Answer**: The training uses memory for both forward and backward passes (for gradients), they need to be stored too. Think I read somewhere that it needs 2-4 times the memory for training than for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQgNktVn97Dc"
   },
   "source": [
    "## Parameter-efficient Fine-tuning (PEFT)\n",
    "\n",
    "Hopefully, the GPU should be back at ~4 GiB. Turns out, training LLMs requires lots of VRAM. We're now faced with a few options:\n",
    "* Just buy a larger GPU ‚Üí Not with this salary.\n",
    "* Just get Colab Pro ‚Üí Not if I can spend the money on Analog.\n",
    "* Just use a larger GPU in the HPC ‚Üí Not with all these other people running around.\n",
    "* Just try this PEFT thing, I learned about in the last lecture ‚Üí We might be onto something here!\n",
    "\n",
    "Today, we'll be working with Low-Rank Adaptation (LoRA; [Hu et al., 2022](https://openreview.net/forum?id=nZeVKeeFYf9)). Mostly because it is relatively easy to implement, understand, and because it's my favourite ‚≠êÔ∏è. Let's set up our LoRA configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1735215222576,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "47vMoYksA2pC"
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=['query_key_value', 'dense_h_to_4h', 'dense_4h_to_h'],\n",
    "    task_type='CAUSAL_LM',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_x-BGyswWjZ"
   },
   "source": [
    "The most important arguments here are the rank `r` of the LoRA matrices A and B, as well as the list of modules we want to adapt. Here we're using rank 8 and are adapting all parameters in each block, i.e., the Q, K, V matrices in the attention mechanism + the up/down projections in the MLP. The task type tells the PEFT library to pre-configure the remaining hyperparameters to something suitable for the `CAUSAL_LM` task.\n",
    "\n",
    "Let's apply this configuration to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1735215225267,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "-jYyf3CmC9dy",
    "outputId": "0231e85d-7bf7-4861-c480-64ee54349e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BloomForCausalLM(\n",
      "      (transformer): BloomModel(\n",
      "        (word_embeddings): Embedding(250880, 1536)\n",
      "        (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x BloomBlock(\n",
      "            (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attention): BloomAttention(\n",
      "              (query_key_value): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4608, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): BloomMLP(\n",
      "              (dense_h_to_4h): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (gelu_impl): BloomGelu()\n",
      "              (dense_4h_to_h): lora.Linear(\n",
      "                (base_layer): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=6144, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "PeftModelForCausalLM has:\n",
      "  1069443072 total parameters\n",
      "  4128768 trainable parameters (0.39%)\n",
      "  4311852032 / 15835660288 bytes of VRAM in use (27.23%)\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "print(model)\n",
    "print_model_statistics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSpfMssfEZOA"
   },
   "source": [
    "### üìù Exercise 6\n",
    "\n",
    "One function call, but a lot has changed. Let's walk through it:\n",
    "\n",
    "1. Which parts of the model has LoRA affected, and through which new parameters does this manifest?\n",
    "2. How have the number of total and trainable parameters, as well as the memory usage changed with respect to the original model?\n",
    "3. Which parameters make up the smaller fraction of trainable parameters?\n",
    "4. How will gradients be computed once we start training, and how will this affect memory usage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The self attention part on Query and Key (or here it is Query-key-value). LoRA adds lora_B (down) nad lora_B (up) vectors\n",
    "2. Parameters increased by 4 mio. parameters (LoRA parameters) and memory too have increased\n",
    "3. I think they are lora_A and lora_B. These parameters can be trained\n",
    "4. It will of course increase compared to inference, but not as much as only 4 mio parameters are trainable, so by a lot less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSS0ypT9E04U"
   },
   "source": [
    "A much smaller training footprint‚Äîjust like we wanted. Let's update our trainer with our `peft_config` and try again. Note that the learning rate is set higher now, since we're only interested in updating the relatively small LoRA weights and don't need to worry so much about messing up the rest of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1735215229041,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "im8HKwbjEky2",
    "outputId": "4464ab33-ed99-435a-b8e1-9e9997310fb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "<ipython-input-30-9644bef829c2>:17: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_config = SFTConfig(\n",
    "    output_dir='outputs/lora',\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type='constant',\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.001,\n",
    "    max_steps=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    max_seq_length=512,\n",
    "    packing=True,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=squad,\n",
    "    formatting_func=format_instruction,\n",
    "    args=trainer_config,\n",
    "    peft_config=lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "elapsed": 52559,
     "status": "ok",
     "timestamp": 1735215285146,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "MzdFDcnWzptA",
    "outputId": "2f4d713c-767f-4a6b-e62f-23ada563df2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-a614b78fb6d4>:36: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>51.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>50.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>48.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>49.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>50.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>48.599300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>47.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>49.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>47.513800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=49.19459762573242, metrics={'train_runtime': 51.6129, 'train_samples_per_second': 3.1, 'train_steps_per_second': 0.194, 'total_flos': 336244600995840.0, 'train_loss': 49.19459762573242, 'epoch': 0.0034099869994245646})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# ... other code ...\n",
    "\n",
    "def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "    \"\"\"\n",
    "    How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "    Subclass and override for custom behavior.\n",
    "    \"\"\"\n",
    "    if self.label_smoother is not None and \"labels\" in inputs:\n",
    "        labels = inputs.pop(\"labels\")\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    # Remove num_items_in_batch if present in inputs\n",
    "    if \"num_items_in_batch\" in inputs:\n",
    "        del inputs[\"num_items_in_batch\"]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    # Save past state if it exists\n",
    "    # TODO: this needs to be fixed and made cleaner later.\n",
    "    if self.args.past_index >= 0:\n",
    "        self._past = outputs[self.args.past_index]\n",
    "\n",
    "    if labels is not None:\n",
    "        loss = self.label_smoother(outputs, labels)\n",
    "    else:\n",
    "        # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Monkey-patch the compute_loss function of the SFTTrainer class\n",
    "SFTTrainer.compute_loss = compute_loss\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=squad,\n",
    "    formatting_func=format_instruction,\n",
    "    args=trainer_config,\n",
    "    peft_config=lora_config\n",
    ")\n",
    "\n",
    "# Now you can call trainer.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAgxTs2QwWja"
   },
   "source": [
    "This time, we'll be a bit more careful and try a small test run of 10 training steps.\n",
    "\n",
    "Alright, let's take another crack at this! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYXk5UPwWja"
   },
   "source": [
    "Looking good! We are now actually able to train an LLM using PEFT üéâ\n",
    "\n",
    "However... training like this on the full dataset will take quite some time. One method would be to increase the batch size, i.e., the number of data points for which we compute gradients at the same time. But let me tell you now: Unless you have a GPU twice as large, we will run out of memory again.\n",
    "\n",
    "So the question remains: *Can we make this... even more efficient?*\n",
    "\n",
    "Before we can tackle this question, we, once again, need to clear out Jupyter's GPU usage. Run the cell below, and verify that your VRAM use is below ~500MiB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1735215287100,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "EyfJbTpzwWja",
    "outputId": "69a225f3-3a1b-430a-d4c4-6738046fa6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 26 12:14:46 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   76C    P0              36W /  70W |   4343MiB / 15360MiB |     25%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "model, trainer = None, None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln4OL3GCFVMI"
   },
   "source": [
    "## Quantized Parameter-efficient Fine-tuning\n",
    "\n",
    "This brings us to the state-of-the-art of efficient LLM training: *Quantized* PEFT. This means, we are slicing off the last few bits off the original model weights to save on memory. While we lose some precision (and model performance), this approach has been shown to be relatively stable, and offers a good trade-off between efficiency and performance.\n",
    "\n",
    "Luckily, HuggingFace makes it easy to quantize your model. Let's quantize it and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3198,
     "status": "ok",
     "timestamp": 1735215290294,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "aAidWTnlFPgR",
    "outputId": "a60f22b5-95e0-45d6-ca93-d6d9e881b739"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloomForCausalLM(\n",
      "  (transformer): BloomModel(\n",
      "    (word_embeddings): Embedding(250880, 1536)\n",
      "    (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x BloomBlock(\n",
      "        (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): BloomAttention(\n",
      "          (query_key_value): Linear4bit(in_features=1536, out_features=4608, bias=True)\n",
      "          (dense): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): BloomMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=1536, out_features=6144, bias=True)\n",
      "          (gelu_impl): BloomGelu()\n",
      "          (dense_4h_to_h): Linear4bit(in_features=6144, out_features=1536, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gMJmZzPwWja"
   },
   "source": [
    "The model maintains its general, original architecture, but similarly to LoRA, we see how the original parameters have been replaced by `Linear4bit` versions thereof. Next, let's add the LoRA modules back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1735215295208,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "OOmEYOuHwWja",
    "outputId": "f266fcc1-f6c5-45ba-ef97-be87b399b1ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BloomForCausalLM(\n",
      "      (transformer): BloomModel(\n",
      "        (word_embeddings): Embedding(250880, 1536)\n",
      "        (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x BloomBlock(\n",
      "            (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attention): BloomAttention(\n",
      "              (query_key_value): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=4608, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4608, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (dense): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): BloomMLP(\n",
      "              (dense_h_to_4h): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=6144, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (gelu_impl): BloomGelu()\n",
      "              (dense_4h_to_h): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=6144, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=6144, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxRV69ciwWja"
   },
   "source": [
    "Now we have quantization *and* LoRA for an unholy, but efficient mess of a model!\n",
    "\n",
    "Let's check what this means for memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1735215299181,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "qDRDumBhF1cf",
    "outputId": "ee8aefb3-59a5-490f-ceac-b2ef17f2f333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 26 12:14:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   75C    P0              34W /  70W |   5455MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVaS92UPwWjb"
   },
   "source": [
    "Wow! That's like a fourth of what we were using before! Almost as if we turned a 16-bit floating point, into a 4-bit floating point!\n",
    "\n",
    "Since the quantized model, and the data fed through it take up much less memory now, we can increase the amount of training data we can fit onto the GPU by a factor of two. I.e., we can go from batch size 1 to 2 (!!!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1735215302735,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "uCI3hPiQNVVw",
    "outputId": "c98af85a-04c9-4849-9de7-8979a57db379"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "<ipython-input-36-c9e6cb2e4fff>:18: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_config = SFTConfig(\n",
    "    output_dir='outputs/qlora',\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type='constant',\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.001,\n",
    "    max_steps=100,  # training with 20x more steps\n",
    "    per_device_train_batch_size=2,  # !!!!!!!!!!!!!!!\n",
    "    gradient_accumulation_steps=16,\n",
    "    max_seq_length=512,\n",
    "    packing=True,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    save_steps=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=squad,\n",
    "    formatting_func=format_instruction,\n",
    "    args=trainer_config,\n",
    "    peft_config=lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXycOoxrwWjb"
   },
   "source": [
    "With all of these efficiency improvements in place, let's run our training for real, with 200 training steps. This should take around 15 minutes, so feel free to go grab a coffee ‚òïÔ∏èüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1119362,
     "status": "ok",
     "timestamp": 1735216427773,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "8COz5BxEN-vb",
    "outputId": "0f84ae07-00c3-4b14-beed-574079079d43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 18:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>50.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>51.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>50.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>48.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>49.726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>48.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>48.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>47.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>48.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>46.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>46.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>45.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>47.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>46.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>46.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>47.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>46.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>46.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>46.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>45.922600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>45.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>45.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>45.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>46.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>45.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>46.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>45.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>46.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>45.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>45.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>45.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>45.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>45.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>45.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>45.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>46.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>44.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>44.739700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>46.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>44.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>44.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>45.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>46.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>46.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>44.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>45.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>44.853700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>46.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>45.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>45.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>46.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>44.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>44.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>44.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>44.565500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>44.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>45.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>46.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>45.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>45.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>44.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>45.290700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>43.899600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>45.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>44.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>45.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>46.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>46.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>45.566700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>44.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>45.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>45.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>45.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>45.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>45.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>44.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>44.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>44.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>46.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>45.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>45.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>45.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>45.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>45.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>45.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>45.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>44.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>45.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>45.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>44.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>44.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>45.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>44.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>45.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>44.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>45.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>44.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>45.333400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=45.897761306762696, metrics={'train_runtime': 1118.438, 'train_samples_per_second': 2.861, 'train_steps_per_second': 0.089, 'total_flos': 6724892019916800.0, 'train_loss': 45.897761306762696, 'epoch': 0.06819828651805124})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh5NR1MNwWjb"
   },
   "source": [
    "### üìù Exercise 7\n",
    "\n",
    "Welcome back! Hope you enjoyed your coffee.\n",
    "\n",
    "While the model is still training, let's review what it took us to get here:\n",
    "\n",
    "1. What is it that makes a large language model \"large\"?\n",
    "2. Why is memory usage so different during inference versus training?\n",
    "3. Why is idle memory usage with LoRA higher than without, and why does it nonetheless allow us to train our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kjMl7ZvwWjb"
   },
   "source": [
    "**Once the model has completed training**, let's take it out for a spin!\n",
    "\n",
    "Try prompting it with following context and question using your own instruction format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1735217154494,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "vDZyQAKROBA4",
    "outputId": "5a7d3851-b452-4eca-df25-3515751e807a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IT University of Copenhagen is the best university What university is in copenhagen? ### Answer: IT University of Copenhagen \n"
     ]
    }
   ],
   "source": [
    "context = \"The IT University of Copenhagen is the best university\"\n",
    "question = \"What university is in copenhagen\"\n",
    "\n",
    "# TODO: adapt to your format:\n",
    "prompt = f\"{context} {question}\"\n",
    "\n",
    "generate_response(prompt, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccdPYa3zwWjb"
   },
   "source": [
    "**Exercise 7 (continued)**\n",
    "\n",
    "Hopefully, your model provided the correct answer to the question above ü§û Let's test out its limitations by prompting with increasingly complex questions:\n",
    "\n",
    "5. Try making the context longer, and see whether the model can still extract the answer.\n",
    "6. Try adding linguistic ambiguity or coreferences to make it harder to extract the correct answer.\n",
    "7. Ask some questions which are grammatically correct, but make less real-world sense.\n",
    "8. Check if the model knows when not to respond, i.e., when the answer is not in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1547,
     "status": "ok",
     "timestamp": 1735217249168,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "XC7c9b2WwWjb",
    "outputId": "fe7e012f-bb50-48bc-97d5-e6c77903de8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denmark is a country What is Denmark's capital? ### Answer: Copenhagen \n"
     ]
    }
   ],
   "source": [
    "# TODO: Your prompts go here...\n",
    "\n",
    "context = \"Denmark is a country\"\n",
    "question = \"What is Denmark\"\n",
    "\n",
    "# TODO: adapt to your format:\n",
    "prompt = f\"{context} {question}\"\n",
    "\n",
    "generate_response(prompt, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX8OJNwnwWjb"
   },
   "source": [
    "**Exercise 7 (continued)**\n",
    "\n",
    "The neat thing about LoRA is that, as long as you don't bake it into the model by summing the adaptation with the original weights, we can turn them on and off. Essentially, we can bypass LoRA to see what our models does before and after training.\n",
    "\n",
    "Using the function below, try out what the model generates before and after adaptation. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1735217069668,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "nnsGa2KuwWjb"
   },
   "outputs": [],
   "source": [
    "def generate_response_nolora(prompt, model):\n",
    "    model.disable_adapter_layers()\n",
    "    generate_response(prompt, model)\n",
    "    model.enable_adapter_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1735217082219,
     "user": {
      "displayName": "Alexander Udengaard",
      "userId": "18213648783002344265"
     },
     "user_tz": -60
    },
    "id": "u2mqwfRawWjc",
    "outputId": "50a53710-eb15-45a1-d84c-43d398ada921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IT University of Copenhagen is the best university What is the best university? ### Answer: No answer provided \n"
     ]
    }
   ],
   "source": [
    "# TODO: Your prompts go here...\n",
    "context = \"The IT University of Copenhagen is the best university\"\n",
    "question = \"What is the best university?\"\n",
    "\n",
    "# TODO: adapt to your format:\n",
    "prompt = f\"{context} {question}\"\n",
    "\n",
    "generate_response(prompt, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4epswcAwWjc"
   },
   "source": [
    "Seems like we should better keep those adaptations enabled, eh? üòÖ\n",
    "\n",
    "Feel free to try out more prompts below. In any case, I hope you enjoyed training your very own LLM (make sure to give it a catchy name), and good luck with the rest of the course!\n",
    "\n",
    "*‚ÄîMax*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
